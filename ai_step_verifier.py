"""
æ™ºèƒ½æ“ä½œæ­¥éª¤æ¯”å¯¹ç³»ç»Ÿï¼ˆAzure GPT-4o è§†è§‰ç†è§£ç‰ˆï¼‰

æ”¯æŒï¼š
1ï¸âƒ£ æ–‡æœ¬è¯­ä¹‰æ¯”å¯¹ï¼ˆSentenceTransformerï¼‰
2ï¸âƒ£ å›¾åƒç›¸ä¼¼åº¦ï¼ˆResNetï¼‰
3ï¸âƒ£ OCRæ£€æµ‹ï¼ˆPaddleOCRï¼‰
4ï¸âƒ£ LLMè§£é‡Š + è§†è§‰ç†è§£ï¼ˆAzure GPT-4oï¼‰
"""

import os
import base64
import numpy as np
from PIL import Image
import torch
from sentence_transformers import SentenceTransformer, util
import torchvision.models as models
import torchvision.transforms as transforms
from paddleocr import PaddleOCR
from openai import AzureOpenAI


# =========================================================
# ğŸ”§ Azure OpenAI é…ç½®ï¼ˆè¯·æ”¹è¿™é‡Œï¼‰
# =========================================================
AZURE_OPENAI_ENDPOINT = "https://YOUR_RESOURCE_NAME.openai.azure.com/"
AZURE_OPENAI_API_KEY = "YOUR_AZURE_KEY"
AZURE_OPENAI_DEPLOYMENT = "gpt-4o"   # GPT-4o æ”¯æŒå›¾åƒè¾“å…¥
API_VERSION = "2024-05-01-preview"

client = AzureOpenAI(
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_key=AZURE_OPENAI_API_KEY,
    api_version=API_VERSION
)


# =========================================================
# 1ï¸âƒ£ æ–‡æœ¬è¯­ä¹‰ç›¸ä¼¼åº¦
# =========================================================
text_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

def semantic_text_similarity(a, b):
    e1 = text_model.encode(a, convert_to_tensor=True)
    e2 = text_model.encode(b, convert_to_tensor=True)
    return util.cos_sim(e1, e2).item()


# =========================================================
# 2ï¸âƒ£ å›¾åƒç‰¹å¾ç›¸ä¼¼åº¦
# =========================================================
image_model = models.resnet18(pretrained=True)
image_model.eval()

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

def image_similarity(path1, path2):
    try:
        i1 = transform(Image.open(path1).convert("RGB")).unsqueeze(0)
        i2 = transform(Image.open(path2).convert("RGB")).unsqueeze(0)
        with torch.no_grad():
            f1 = image_model(i1).flatten()
            f2 = image_model(i2).flatten()
        return torch.nn.functional.cosine_similarity(f1, f2, dim=0).item()
    except Exception:
        return 0.0


# =========================================================
# 3ï¸âƒ£ OCRè¯†åˆ«
# =========================================================
ocr = PaddleOCR(use_angle_cls=True, lang='ch')

def ocr_text_match(img_path, expected):
    try:
        result = ocr.ocr(img_path, cls=True)
        text = " ".join([r[1][0] for r in result[0]])
        return (expected in text), text
    except Exception:
        return False, ""


# =========================================================
# 4ï¸âƒ£ LLM å›¾åƒ+æ–‡å­—ç†è§£ï¼ˆGPT-4oï¼‰
# =========================================================
def llm_understand_image(step_text, image_path):
    """
    ç”¨ GPT-4o åˆ¤æ–­ï¼šå›¾ç‰‡å†…å®¹æ˜¯å¦ç¬¦åˆæ–‡å­—æè¿°
    """
    with open(image_path, "rb") as f:
        img_b64 = base64.b64encode(f.read()).decode("utf-8")

    prompt = f"""
è¯·é˜…è¯»ä¸‹é¢çš„æ­¥éª¤æè¿°ï¼Œå¹¶åˆ†æå›¾ç‰‡æ˜¯å¦è¡¨ç°äº†è¯¥æ“ä½œã€‚

æ­¥éª¤æè¿°ï¼š
{step_text}

è¯·å›ç­”ï¼š
- å›¾ç‰‡æ˜¯å¦ç¬¦åˆè¯¥æè¿°ï¼ˆæ˜¯/å¦ï¼‰
- ç®€è¦è¯´æ˜ç†ç”±ï¼ˆä¸­æ–‡ï¼‰
"""

    try:
        response = client.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè§†è§‰ç†è§£ä¸“å®¶ï¼Œæ“…é•¿åˆ†ææˆªå›¾æ˜¯å¦åŒ¹é…æ“ä½œæè¿°ã€‚"},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": f"data:image/png;base64,{img_b64}"
                        }
                    ]
                }
            ],
            max_tokens=300,
            temperature=0.2
        )

        reply = response.choices[0].message.content.strip()
        match = "æ˜¯" in reply or "ç¬¦åˆ" in reply
        return match, reply

    except Exception as e:
        return False, f"[LLMé”™è¯¯: {e}]"


# =========================================================
# 5ï¸âƒ£ ä¸»é€»è¾‘
# =========================================================
def compare_steps_ai_reason(standard_steps, actual_steps, text_threshold=0.75):
    results = []

    for i, (std, act) in enumerate(zip(standard_steps, actual_steps), start=1):
        text_sim = semantic_text_similarity(std["text"], act["text"])
        img_sim = None
        ocr_match = True
        reason = ""

        # å¦‚æœæ ‡å‡†æ­¥éª¤æœ‰å›¾åƒ
        if std.get("image_path") and act.get("image_path"):
            img_sim = image_similarity(std["image_path"], act["image_path"])

        # å¦‚æœæ ‡å‡†æ­¥éª¤æœ‰OCRè¦æ±‚
        if std.get("ocr_text") and act.get("image_path"):
            ocr_match, ocr_text = ocr_text_match(act["image_path"], std["ocr_text"])

        # ğŸ§  å¦‚æœæ ‡å‡†æ­¥éª¤æ²¡æœ‰å›¾åƒä½†æœ‰æè¿°ï¼Œä½¿ç”¨ GPT-4o åˆ¤æ–­æˆªå›¾æ˜¯å¦ç¬¦åˆ
        if not std.get("image_path") and act.get("image_path"):
            llm_match, llm_reason = llm_understand_image(std["text"], act["image_path"])
            if not llm_match:
                results.append({
                    "step": i,
                    "result": False,
                    "reason": f"âŒ æ­¥éª¤ {i}ï¼šæˆªå›¾æœªè¡¨ç°å‡º '{std['text']}' çš„æ“ä½œã€‚\nLLMåˆ†æï¼š{llm_reason}"
                })
                continue

        # æ­£å¸¸æ–‡æœ¬ç›¸ä¼¼åº¦åˆ¤å®š
        if text_sim < text_threshold or not ocr_match:
            results.append({
                "step": i,
                "result": False,
                "reason": f"âŒ æ­¥éª¤ {i}ï¼šæ–‡å­—æˆ–OCRæœªåŒ¹é…ã€‚\næ ‡å‡†ï¼š{std['text']} å®é™…ï¼š{act['text']}"
            })
            continue

        results.append({
            "step": i,
            "result": True,
            "reason": f"âœ… æ­¥éª¤ {i} åŒ¹é…æˆåŠŸ"
        })

    return results


# =========================================================
# 6ï¸âƒ£ ç¤ºä¾‹
# =========================================================
if __name__ == "__main__":
    standard_steps = [
        {"text": "å³é”®æ–°å»ºæ–‡ä»¶å¤¹", "image_path": None},
        {"text": "æ‰“å¼€æµè§ˆå™¨å¹¶è¿›å…¥ http://example.com", "image_path": None}
    ]
    actual_steps = [
        {"text": "ç”¨æˆ·å³é”®ç‚¹å‡»", "image_path": "screenshots/right_click.png"},
        {"text": "ç”¨æˆ·æ‰“å¼€æµè§ˆå™¨", "image_path": "screenshots/browser.png"}
    ]

    result = compare_steps_ai_reason(standard_steps, actual_steps)
    for r in result:
        print(r)
