import os
import cv2
import numpy as np
from PIL import Image
from tqdm import tqdm
from paddleocr import PaddleOCR
from transformers import CLIPProcessor, CLIPModel
import torch
import json


# ===== å‚æ•°é…ç½® =====
FRAME_DIFF_THRESHOLD = 12       # å›¾åƒåƒç´ å˜åŒ–é˜ˆå€¼
MIN_FRAME_INTERVAL = 1.0        # æ¯éš”å‡ ç§’æ£€æµ‹ä¸€å¸§
SIMILARITY_THRESHOLD = 0.25     # åŒ¹é…ç›¸ä¼¼åº¦é˜ˆå€¼
OUTPUT_DIR = "output"


# ===== åˆå§‹åŒ–æ¨¡å‹ =====
print("ğŸ§  åŠ è½½ CLIP ä¸ OCR æ¨¡å‹ä¸­...")
device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
ocr_engine = PaddleOCR(use_angle_cls=True, lang='ch')


# ===== OCR æå–æ–‡å­— =====
def ocr_extract_text(pil_image: Image.Image) -> str:
    np_img = np.array(pil_image.convert("RGB"))
    result = ocr_engine.ocr(np_img, cls=True)
    texts = []
    for line in result:
        for _, (text, conf) in line:
            if conf > 0.5:
                texts.append(text)
    return " ".join(texts)


# ===== æå–å…³é”®å¸§ï¼ˆå›¾åƒå·®å¼‚ + OCRæ–‡å­—å·®å¼‚ï¼‰ =====
def extract_keyframes(video_path: str, out_dir: str = None,
                      diff_threshold=FRAME_DIFF_THRESHOLD,
                      min_interval=MIN_FRAME_INTERVAL,
                      ocr_diff=True):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    frame_interval = int(max(1, fps * min_interval))
    keyframes = []
    prev_gray = None
    prev_text = ""
    frame_idx = 0

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    pbar = tqdm(total=total_frames, desc="ğŸï¸ æå–å…³é”®å¸§")

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_idx % frame_interval != 0 and prev_gray is not None:
            frame_idx += 1
            pbar.update(1)
            continue

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        diff = np.mean(cv2.absdiff(gray, prev_gray)) if prev_gray is not None else 999

        pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        text = ""
        if ocr_diff and frame_idx % int(fps * 2) == 0:
            try:
                text = ocr_extract_text(pil)
            except Exception:
                text = ""

        text_changed = (ocr_diff and len(text) > 0 and text != prev_text)
        take = (diff > diff_threshold) or text_changed or prev_gray is None

        if take:
            keyframes.append((frame_idx, pil))
            prev_gray = gray
            prev_text = text

            if out_dir:
                os.makedirs(out_dir, exist_ok=True)
                pil.save(os.path.join(out_dir, f"frame_{frame_idx:06d}.jpg"))

        frame_idx += 1
        pbar.update(1)

    pbar.close()
    cap.release()
    return keyframes


# ===== CLIP åŒ¹é…å‡½æ•° =====
def clip_similarity(image: Image.Image, text: str) -> float:
    inputs = clip_processor(text=[text], images=image, return_tensors="pt", padding=True).to(device)
    outputs = clip_model(**inputs)
    logits_per_image = outputs.logits_per_image
    return logits_per_image.softmax(dim=1).max().item()


# ===== ä¸»é€»è¾‘ï¼šåŒ¹é…æ“ä½œæ–‡æ¡£æ­¥éª¤ =====
def check_operation(video_path: str, standard_steps: list, output_dir=OUTPUT_DIR):
    keyframes = extract_keyframes(video_path, out_dir=os.path.join(output_dir, "frames"))
    results = []

    for idx, step in enumerate(tqdm(standard_steps, desc="ğŸ§© åŒ¹é…æ“ä½œæ­¥éª¤")):
        step_text = step.get("description", "")
        step_img = step.get("image", None)

        best_score = 0
        best_frame_idx = None
        matched = False

        for frame_idx, frame_img in keyframes:
            score = clip_similarity(frame_img, step_text)
            if score > best_score:
                best_score = score
                best_frame_idx = frame_idx

        matched = best_score >= SIMILARITY_THRESHOLD
        reason = "æ“ä½œä¸€è‡´ âœ…" if matched else "æœªæ£€æµ‹åˆ°å¯¹åº”æ“ä½œ âŒ"

        results.append({
            "step": idx + 1,
            "expected_description": step_text,
            "clip_score": round(best_score, 3),
            "matched": matched,
            "reason": reason,
            "frame_index": best_frame_idx
        })

    save_html_report(results, output_dir)
    return results


# ===== HTML æŠ¥å‘Šç”Ÿæˆ =====
def save_html_report(results, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    html_path = os.path.join(output_dir, "report.html")

    with open(html_path, "w", encoding="utf-8") as f:
        f.write("<html><head><meta charset='utf-8'><title>ç½‘é¡µæ“ä½œæ¯”å¯¹æŠ¥å‘Š</title></head><body>")
        f.write("<h1>ç½‘é¡µæ“ä½œæ¯”å¯¹æŠ¥å‘Š</h1>")
        for r in results:
            color = "green" if r["matched"] else "red"
            f.write(f"<h3 style='color:{color}'>æ­¥éª¤ {r['step']} - {r['reason']}</h3>")
            f.write(f"<p>æè¿°ï¼š{r['expected_description']}<br>")
            f.write(f"ç›¸ä¼¼åº¦ï¼š{r['clip_score']}</p><hr>")
        f.write("</body></html>")

    print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼š{html_path}")


# ===== ç¤ºä¾‹å…¥å£ =====
if __name__ == "__main__":
    # è¿™é‡Œä¸¾ä¾‹ï¼šæ ‡å‡†æ“ä½œæ–‡æ¡£ç»“æ„
    # ä½ å¯ä»¥æŠŠå®ƒæ”¹æˆä» JSON æ–‡ä»¶è¯»å–
    standard_steps = [
        {"description": "æ‰“å¼€æµè§ˆå™¨å¹¶è®¿é—®ç™»å½•é¡µ"},
        {"description": "è¾“å…¥ç”¨æˆ·åå¯†ç å¹¶ç‚¹å‡»ç™»å½•"},
        {"description": "åœ¨é¦–é¡µç‚¹å‡»â€˜æ•°æ®ç®¡ç†â€™é€‰é¡¹"},
        {"description": "è¿›å…¥æŸ¥è¯¢ç•Œé¢å¹¶è¾“å…¥æŸ¥è¯¢æ¡ä»¶"},
        {"description": "ç‚¹å‡»â€˜æŸ¥è¯¢â€™æŒ‰é’®æŸ¥çœ‹ç»“æœ"}
    ]

    video_path = "your_video.mp4"  # è¿™é‡Œæ¢æˆä½ çš„å½•å±æ–‡ä»¶è·¯å¾„
    results = check_operation(video_path, standard_steps)
    print(json.dumps(results, ensure_ascii=False, indent=2))
